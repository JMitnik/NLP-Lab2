\section{Introduction}
% Pages-size: 1
\label{sec: intro}
Sentiment analysis is a sub-domain within the field of Natural Language
Processing, in which a statistical model infers the direction and magnitude of
emotions from text. Successful inference depends on building appropriate
language models which can model the structural dependencies and semantics within
the sentences of a language efficiently. One such structural dependency is the
context of a phrase. Context is derived from factors such as word order, or even
the granularity of how much data the model has been trained on. Insufficient
context might lead to not learning negations, for instances. The goal of this
paper is to investigate the role of representations of context. The importance
of word order and data granularity will be investigated by studying a number of
different models, each varying in how granular information is parsed.

    \textit{1. How important is word order for the task of classification of
    sentiment?} Some models, such as the traditional Bag of Words, do not
    capture word order. Long short-term memory networks (LSTM), maintain a
    memory state of recent words encountered to incorporate word-order of short
    and long distances. This paper initially believes that models which maintain
    and understand word-order will perform better. Findings suggest (\ref{sec:
    results}) that including word-order improves performance significantly.

    \textit{2. Does tree structure improve the accuracy as opposed to sequential
    learning?} Vanilla LSTM takes in words in a sequential manner. One
    alternative to LSTM is Tree LSTM, which uses a tree structure to obtain
    phrase representation both globally and locally, to enhance the context
    available for the model. This paper seeks to investigate whether aggregating
    context structurally will increase the performance of the inference on the
    test set. The original belief is that this is be the case, but final results
    indicate no significant difference between LSTM and Tree LSTM.
    
    \textit{3. Performance on supervising sentiment per node?}
    This paper is seeking to investigate whether adding additional supervision
    on intermediate words or phrases will help the model capture the more
    context-related sentiment. The initial hypothesis of this paper argues that the
    supervision of individual nodes might decrease performance, due to less context
    to account for negating sentiment, for instance. Later findings suggest no
    significant difference when supervising per node.

    \textit{4. Will the performance vary much on different sentence length across models?}
    A model may behave very differently to sentences of different ranges of
    length, due to a higher probability of encountering sentiment contradictions
    in longer sentences, and class imbalances on the sentence-length in the
    training-set.  Therefore it is a crucial problem whether a model performs
    evenly on different length sentences and whether some model performs better
    on capturing sentiments contexts in longer timescale. Initially, this paper
    argues that Tree LSTM might perform the best, as it processes the context in
    a hand-crafted structural way, thus more efficient when dealing with
    long-scale semantics.

    \textit{5. How does N-ary Tree LSTM compare to the Child-Sum Tree LSTM?}
    The N-ary tree-LSTM differs with Child-sum tree-LSTM in that the latter is
    agnostic on the order of the child nodes. On implementation details, the
    N-ary tree-LSTM has different gate parameters for each child, while
    Child-sum use the same parameter for all the gate including forget gate. In
    most cases, N-ary tree-LSTM first project then sum, while the Child-sum
    tree-LSTM do the opposite. Hence they are suitable for different tasks: the
    N-ary tree-LSTM is suitable to those trees where child nodes are ordered,
    and the Child-sum tree-LSTM should work well on the order of child nodes
    does not contribute to the context. In the sentiment classification task, we
    need N-ary tree-LSTM since the word order is important to context
    extraction.
