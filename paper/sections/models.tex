\section{Models}
\label{sec: models}
\subsection{BOW, CBOW and Deep CBOW}
The BOW sums vectors of size $R^{D}$, where D is the number of sentiment labels,
from very negative to very positive. D is 5 in this paper. For each word, an
index of D represents the likelihoods $p(label|word)$ for each label based on
word-count in training, and after summing these likelihood vectors for each
word, the label with the highest likelihood is the eventual prediction. The CBOW
sums vectors of a dimension 300, which is an arbitrary size to represent some
semantical learnable properties of a word. The CBOW uses a perceptron by
performing an affine transformation to the D-dimensional output, which
corresponds to the output label. Deep CBOW is an extension to this technique,
and converts the single-layer perceptron to a multi-layer perceptron. The model
performs a linear transformation of the input ($R^{300}$) to a hidden layer
size, which is in $R^{100}$. An activation function using the hyperbolic tangent
(tanh) ensures non-linearity. After another linear transformation to the same
$R^{100}$ and another tanh, the model maps these results to $R^D$. In this
paper, DCBOW is both trained with a newly initialized word-embedding, and a
pretrained word embedding (DCBOW-PT).
\subsubsection{LSTM and Tree LSTM}
The LSTM and N-Ary Tree LSTM Architecture are similar in that their calculations
mostly consist of the same steps, and the comparison can be made that LSTM is a
Tree LSTM with one child-node for each node. Each gate within a cell consists of
their own unique weight, $U$ and $W$, which correspond respectively to the
weights in relation to a child k, and in relation to the input $x$. Each gate
also encompasses a bias, $b$. Each gate follows a similar affine transformation
of the prior hidden state and input. These can be found in
\cite{tai2015improved}, 8-14, and correspond to Vanilla LSTM's formulas if
$max_k=1$. Initially, upon receiving an input in time the forget gate $f_{jk}$
is responsible for deciding how much, for each child k, the child's hidden state
$h_jk$, is interpreted into the parent (j)'s cell state: $f_jk =
\sigma(W^{(f)}x_j+\sum\limits^{N}_{l=1}U_{kl}^{(f)}*h_{jl}+b^{(f)}$. A
combination of the input vector,
$i_j=\sigma(W^{(i)}x_j+\sum\limits^{N}_{l=1}U_{l}^{(i)}*h_{jl}+b^{(i))$ and
candidate selection, can be used to change the value of the unit's cell
state. Each of the children's cell states with a product conjunction with the
current unit's forget gate, are combined and result into










For Tree SLTM by performing the calculation $f_t = \sigma(W_f * [h_{t-1}, x_t]
+ b_f)$. Next, the input gate decides how much of each value of the cell state
to update, $i_t = \sigma(W_i * [h_{t-1}, x_t] + b_i)$. An intermediate step
calculates the actual candidate selection of the input, by $C_t = tanh(W_c *
[h_{t-1}, x_t] + b_c)$. These two can then be implemented to change the value of
the cell state. The cell state is then finally accessed by an output gate, $o_t
= \sigma(W_o * [h_{t-1}, x_t] + b_o)$, and multiplied by the tanh of the
candidates, to calculate the new hidden state.














\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lllllllll}
    BOW  & CBOW & DCBOW & DCBOW-PT & LSTM & Mini-LSTM & T-LSTM & ST-LSTM &CS-LSTM \\
    19$\pm$ 4 & 28$\pm$ 7 & 25$\pm$ 2 & 44$\pm$ 1 & \textbf{47$\pm$ 1} & 46$\pm$ 0.4 & \textbf{47$\pm$ 1} & 45$\pm$ 1& \textbf{47$\pm$ 0.4}
    \end{tabular}
    \label{table: results-acc}
    \caption{The mean and variance of test-set accuracies of three runs over 3 seeds(shown in percent)}.
\end{table*}


\begin{table*}[t]
    \small
    \begin{tabular}{llllllllll}
                     & BOW     & CBOW     & DCBOW  & DCBOW-PT   & LSTM    &
                     Mini-LSTM & T-LSTM & ST-LSTM & CS-LSTM\\
        CBOW         & 0.18  &   -   & 0.015  & 0.048 &0.0075&8.2E-5 & 2.0E-6 &3.3E-10    & 2.5E-7\\
        DCBOW    & 0.39   &  0.015   &  -    &  0.57  & 0.15  & 2.3E-3  & 4.0E-5 & 8.8E-9     & 1.1E-4 \\
        DCBOW-PT & 0.17  &   0.048  &   0.57  &   -  &  0.58 &  0.093  &   0.015  & 3.6E-4 & 0.02 \\
        LSTM         & 2.1E-4 &  7.6E-3 &  0.15  &  0.58 & -  &  0.32  &  0.075  & 4.0E-3    &0.1  \\
        Mini-LSTM    & 1.6E-5 &  8.2E-5 &  2.4E-3  &  0.093  &  0.32  &   -    &    0.48   &  0.057  & 0.5   \\
        T-LSTM    & 5.3-7  & 2.0E-6 & 4.0E-5 &  0.015  & 0.07  &   0.48   &    -    &   0.24    &0.9 \\
        ST-LSTM & 8.3-19  &  3.3-10  &  8.8-9  & 3.6E-4 & 4.0E-3 &  0.057   &    0.24   &   -     & 0.2  
        \end{tabular}
    \label{table: sign}
    \caption{Significance of sign test across all models pairs.}
\end{table*}