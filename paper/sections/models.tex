\vspace{-5pt}
\section{Models}
\label{sec: models}
\vspace{-5pt}
\subsection{BOW, CBOW and Deep CBOW}
The BOW sums vectors of size $R^{D}$, where D is the number of sentiment labels,
from very negative to very positive. D is 5 in this paper. For each word, an
index of D represents the likelihoods $p(label|word)$ for each label based on
word-count in training, and after summing these likelihood vectors for each
word, the label with the highest likelihood is the eventual prediction. The CBOW
sums vectors of a dimension 300. The CBOW uses a perceptron by performing an
affine transformation to the D-dimensional output, which corresponds to the
output label. Deep CBOW is an extension to this technique, and converts the
single-layer perceptron to a multi-layer perceptron. The model performs a linear
transformation of the input ($R^{300}$) to a hidden layer size, which is in
$R^{100}$. An activation function using the hyperbolic tangent (tanh) ensures
non-linearity. After another linear transformation to the same $R^{100}$ and
another tanh, the model maps these results to $R^D$. In this paper, DCBOW is
both trained with a newly initialized word-embedding, and a pretrained word
embedding (DCBOW-PT).
\vspace{-5pt}
\subsubsection{LSTM and Tree LSTM}
Tree LSTM is a generalization of vanilla LSTM on series with tree structure, and
N-ary Tree LSTM act exact the same with vanilla LSTM when acting upon uni-tree,
namely series with explicit causal arrows. LSTM is consists of one iterated
cell, within the cell exist 5 gates: $f_{tk}$, $i_t$, $c_t$, $h_t$, $o_t$. Each
gate consists of its own unique weight, $U_k$ and $W$, which correspond
respectively to the weights in relation to the $k$th child, and in relation to
the input $x$. Each gate also encompasses a bias, $b$. Each gate follows a
similar affine transformation of the prior hidden state and input. By
compositing the gates and previous states together, we can gain the states to be
passed to the next time step. At the last time step, we take the output state
$o_T$ as the representation of the whole sentence and pass it to a MLP to
perform classification.
\vspace{-8pt}
% Initially, upon receiving an input in time the forget gate $f_{jk}$
% is responsible for deciding how much, for each child k, the child's hidden state
% $h_jk$, is interpreted into the parent (j)'s cell state: $f_jk =
% \sigma(W^{(f)}x_j+\sum\limits^{N}_{l=1}U_{kl}^{(f)}*h_{jl}+b^{(f)}$. A
% combination of the input vector,
% $i_j=\sigma(W^{(i)}x_j+\sum\limits^{N}_{l=1}U_{l}^{(i)}*h_{jl}+b^{i}$ and
% candidate selection, can be used to change the value of the unit's cell
% state. Each of the children's cell states with a product conjunction with the
% current unit's forget gate, are combined and result into
% These can be found in
% \cite{tai2015improved}, 8-14, and correspond to Vanilla LSTM's formulas if
% $max_k=1$.



% For Tree LSTM by performing the calculation $f_t = \sigma(W_f * [h_{t-1}, x_t]
% + b_f)$. Next, the input gate decides how much of each value of the cell state
% to update, $i_t = \sigma(W_i * [h_{t-1}, x_t] + b_i)$. An intermediate step
% calculates the actual candidate selection of the input, by $C_t = tanh(W_c *
% [h_{t-1}, x_t] + b_c)$. These two can then be implemented to change the value of
% the cell state. The cell state is then finally accessed by an output gate, $o_t
% = \sigma(W_o * [h_{t-1}, x_t] + b_o)$, and multiplied by the tanh of the
% candidates, to calculate the new hidden state.














\begin{table}[t!]
    \begin{center}
    \small
    \begin{tabular}{|c|l|r|l|}
        \hline \bf Models & \bf HD & \bf LR &\bf NoP \\ \hline
        BOW & None & 0.0005 &0\\
        CBOW & None & 0.0005 & 1500\\
        DCBOW(-PT)& 100 & 0.0005 &40500\\
        LSTM&168 & 0.0003 &316680\\
        Mini-LSTM&168 & 0.0002&316680\\
        (S)T-LSTM&150 & 0.0002&315750\\
        CS-LSTM&150 & 0.0002& 180750\\
        \hline
    \end{tabular}
    \end{center}
    \caption{Hyper-parameters of each model. Where HD refers to hidden dimension, LR refers to learning rate and NoP stands for number of parameters.}
    \label{tab:parameters}
\end{table}

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lllllllll}
    BOW  & CBOW & DCBOW & DCBOW-PT & LSTM & Mini-LSTM & T-LSTM & ST-LSTM &CS-LSTM \\
    19$\pm$ 4 & 28$\pm$ 7 & 25$\pm$ 2 & 44$\pm$ 1 & \textbf{47$\pm$ 1} & 46$\pm$ 0.4 & \textbf{47$\pm$ 1} & 45$\pm$ 1& \textbf{47$\pm$ 0.4}
\end{tabular}
\caption{The mean and variance of test-set accuracies of three runs over 3 seeds(shown in percent)}.
\label{table: results-acc}
\end{table*}


\begin{table*}[t]
    \small
    \begin{tabular}{llllllllll}
                     & BOW     & CBOW     & DCBOW  & DCBOW-PT   & LSTM    &
                     Mini-LSTM & T-LSTM & ST-LSTM & CS-LSTM\\
        CBOW         & 0.18  &   -   & 0.015  & 0.048 &0.0075&8.2E-5 & 2.0E-6 &3.3E-10    & 2.5E-7\\
        DCBOW    & 0.39   &  0.015   &  -    &  0.57  & 0.15  & 2.3E-3  & 4.0E-5 & 8.8E-9     & 1.1E-4 \\
        DCBOW-PT & 0.17  &   0.048  &   0.57  &   -  &  0.58 &  0.093  &   0.015  & 3.6E-4 & 0.02 \\
        LSTM         & 2.1E-4 &  7.6E-3 &  0.15  &  0.58 & -  &  0.32  &  0.075  & 4.0E-3    &0.1  \\
        Mini-LSTM    & 1.6E-5 &  8.2E-5 &  2.4E-3  &  0.093  &  0.32  &   -    &    0.48   &  0.057  & 0.5   \\
        T-LSTM    & 5.3-7  & 2.0E-6 & 4.0E-5 &  0.015  & 0.07  &   0.48   &    -    &   0.24    &0.9 \\
        ST-LSTM & 8.3-19  &  3.3-10  &  8.8-9  & 3.6E-4 & 4.0E-3 &  0.057   &    0.24   &   -     & 0.2  
        \end{tabular}
        \caption{Significance of sign test across all models pairs.}
        \label{table: sign}
\end{table*}