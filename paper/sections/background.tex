\section{Background}
\label{sec: background}

\subsubsection{Word Embeddings}
Word embedding is a fundamental technique to represent words in an efficient
way. The basic idea is to cast all words in the vocabulary to a low dimensional
real space in a fashion that the more close two words semantically, the more
close they are in terms of Euclidean distance in the embedding space. There are
several embedding types such as word2vec\cite{mikolov2013distributed} and
GloVe\cite{pennington2014glove}, all based on estimating the neighbor words
distribution. We use GloVe in this paper.
 
\subsubsection{BOW, CBOW and Deep CBOW}
The bag of words (BOW) model uses word-count to predict the most likely
sentiment class. This language model has no representation of word-order and
thus has been considered a simple but not sufficient representation to
effectively capture word-order. The Continuous bag of word (CBOW)
model\cite{DBLP:journals/corr/abs-1301-3781} is an extension of the BOW by
changing the dimensionality of each of its embeddings to an arbitrary size to
house some semantic representation for each word. Furthermore, to ensure proper
calculations, the model learns the corresponding mapping of these semantic
representations to output classes by training a one-layer perceptron. The Deep
CBOW model uses the benfit of a multi-layer perceptron to encode more
information about a word's relationship to the supervising signal (for instance,
sentiment). These models result in gradients on which an optimizer can perform
gradient descent. To derive the gradient from these perceptrons to perform
stochastic gradient descent on, the Pytorch library can perform automatic
differentiation\cite{paszke2017automatic}.

\subsubsection{Long-short term network}
Whereas the  previous models had no representation of word-order, Long-short
term network models (LSTM) use unit cells to maintain a 'hidden state' and 'cell
state' across temporal data, such as a sentence. Introduced in
1997\cite{hochreiter1997long}, LSTMs are a responsive to the vanishing gradient
problem found in Recurrent Neural Networks\cite{bengio1994learning}. This
problem occurs when the gradient vector exponentially decays over the length of
a sentence: this causes an effective loss in long-term memory. LSTM, maintains
an additional cell state, $c_t$, across cells, by regulating any read or write
operation on this cell-sate using regulating gates. The hidden state contains
more contemporary relevant information about the data, such as active sentiment.

\subsubsection{Tree LSTM}
While Recurrent Neural Networks and LSTMs process previous unit cells
sequentially, Tree LSTM cells \cite{DBLP:journals/corr/TaiSM15}
\cite{DBLP:journals/corr/ZhuSG15} \cite{DBLP:journals/corr/LeZ15} contain
multiple children cells from which each cell can obtain past knowledge. A paper
by [Tai et al] suggest two variants on how these child cells can be implemented:
Child-Sum Tree LSTMs and N-Ary Trees. For a Child-sum LSTM cell, a summation of
all k-child hidden states is calculated. Each gate vector in this variant is
calculated similarly to regular LSTMs, except the LSTM stores an additional
weight matrix per gate for how to incorporate all the previous solutions, and
order of children is not deemed important. For N-Ary trees, a unit cell
additionally receives a previous memory cell. Instead of summing the previous
hidden states beforehand, this variant does that in each gate with a weight
matrix per assigned child. Kai et al state that this allows for more granular
calculations leading to more fine-tuned results.