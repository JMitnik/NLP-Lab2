{
  "nbformat_minor": 1,
  "cells": [
    {
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "plt.style.use('default')"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "class MutePrint:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stdout_restore = sys.stdout\n",
        "    # Disable\n",
        "\n",
        "    def blockPrint(self):\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    # Restore\n",
        "    def enablePrint(self):\n",
        "        sys.stdout = self.stdout_restore\n",
        "mute = MutePrint()\n",
        "# %%\n",
        "!wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip -O trainDevTestTrees_PTB.zip\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "!pip install pytreebank\n",
        "import pytreebank\n",
        "# %%\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig - p | grep cudart.so | sed - e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" ."
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "!wget -q https://github.com/JMitnik/NLP-Lab2/raw/cg/main.py -O ./main.py"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "mute.blockPrint()\n",
        "from main import *\n",
        "mute.enablePrint()"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "def get_subtree_dataset():\n",
        "    '''\n",
        "    extract all subtrees together to the exact form of the `train_data` used in last ipynb\n",
        "    args: None\n",
        "    returns: a list contains three list of Examples, each of them corresponds to one of 'train', 'test', 'dev' set\n",
        "    '''\n",
        "    dataset = pytreebank.load_sst(\"./\")\n",
        "    datasets = dataset.values()\n",
        "    print(datasets.keys())\n",
        "    results = []\n",
        "    for D in datasets:\n",
        "        result = []\n",
        "        for tree in D:\n",
        "            tree.lowercase()\n",
        "            for c in tree.all_children():\n",
        "                sc = str(c)\n",
        "                trans = transitions_from_treestring(sc)\n",
        "                label = self.label\n",
        "                tree = c\n",
        "                tokens = tokens_from_treestring(sc)\n",
        "                result.append(Example(tokens=tokens, tree=tree, label=label, transitions=trans))\n",
        "        results.append(result)\n",
        "    return results"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "subtree_train_data, subtree_dev_data, subtree_test_data = get_subtree_dataset()"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    },
    {
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "class Experiment():\n",
        "\n",
        "    def __init__(self, model, optimizer, *args, **kwargs):\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "    def train(self):\n",
        "        path = \"{}.pt\".format(kwargs['exp_name'] if 'exp_name' in kwargs or self.model.__class__.__name__)\n",
        "        if os.path.exists(path):\n",
        "            ckpt= torch.load(path)\n",
        "            self.model.load_state_dict(ckpt[\"state_dict\"])\n",
        "            return\n",
        "        self.losses, self.accs = train_model(*self.args, **self.kwargs)\n",
        "    def eval(self, eval_fn=None, data=test_data, **kwargs):\n",
        "        if eval_fn is None:\n",
        "            if not ('eval_fn' in kwargs):\n",
        "                eval_fn = simple_evaluate\n",
        "            else:\n",
        "                eval_fn = self.kwargs['eval_fn'] \n",
        "        return eval_fn(self.model, data, **kwargs)\n",
        "\n",
        "    def plot(self):\n",
        "        plt.plot(self.losses)\n",
        "        plt.plot(self.accs)\n",
        "        return\n",
        "    def get_accuracy():\n",
        "        return self.accs\n",
        "\n",
        "    def get_losses():\n",
        "        return self.losses\n",
        "\n",
        "    results = do_train(tree_model)\n",
        "    acc, loss = results\n",
        "    plt.plot(acc)\n",
        "# %%\n",
        "def prepare_subtreelstm_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Returns sentences reversed (last word first)\n",
        "  Returns transitions together with the sentences.  \n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "    \n",
        "  # vocab returns 0 if the word is not there\n",
        "  # NOTE: reversed sequence!\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
        "  \n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "  \n",
        "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
        "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
        "  transitions = np.array(transitions)\n",
        "  transitions = transitions.T  # time-major\n",
        "  \n",
        "  return (x, transitions), y\n",
        "# %%\n",
        "# build all the experiments by feeding corresponding parameters\n",
        "# cant think of cleaner way to do it :(\n",
        "xargs_bow = dict(num_iterations=30000, print_every=1000, eval_every=1000)\n",
        "optimizer = optim.Adam(bow_model.parameters(), lr=0.0005)\n",
        "bow_exp = Experiment(bow_model, optimizer, **xargs_bow)\n",
        "\n",
        "optimizer = optim.Adam(cbow_model.parameters(), lr=0.0005)\n",
        "cbow_exp = Experiment(cbow_model, optimizer, **xargs_bow)\n",
        "\n",
        "optimizer = optim.Adam(deep_cbow_model.parameters(), lr=0.0005)\n",
        "deep_cbow_exp = Experiment(deep_cbow_model, optimizer, **xargs_bow)\n",
        "\n",
        "optimizer = optim.Adam(pt_deep_cbow_model.parameters(), lr=0.0005)\n",
        "deep_cbow_exp = Experiment(pt_deep_cbow_model, optimizer, num_iterations=30000,\n",
        "      print_every=1000, eval_every=1000)\n",
        "\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
        "lstm_exp = Experiment(lstm_model, optimizer, num_iterations=25000, print_every=250, eval_every=1000)\n",
        "\n",
        "optimizer = optim.Adam(tree_model.parameters(), lr=2e-4)\n",
        "tree_lstm_exp = Experiment(tree_model, optimizer, num_iterations=30000, \n",
        "      print_every=250, eval_every=250,\n",
        "      prep_fn=prepare_treelstm_minibatch,\n",
        "      eval_fn=evaluate,\n",
        "      batch_fn=get_minibatch,\n",
        "      batch_size=25, eval_batch_size=25)\n",
        "\n",
        "# build a new tree lstm for feeding subtree\n",
        "sub_tree_model = TreeLSTMClassifier(\n",
        "    len(v.w2i), 300, 150, len(t2i), v)\n",
        "\n",
        "with torch.no_grad():\n",
        "  sub_tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "  sub_tree_model.embed.weight.requires_grad = False\n",
        "optimizer = optim.Adam(sub_tree_model.parameters(), lr=2e-4)\n",
        "sub_tree_lstm_exp = Experiment(sub_tree_model, optimizer, num_iterations=30000,\n",
        "                           print_every=250, eval_every=250,\n",
        "                           prep_fn=prepare_treelstm_minibatch,\n",
        "                           eval_fn=evaluate,\n",
        "                           batch_fn=get_minibatch,\n",
        "                           batch_size=25, eval_batch_size=25, exp_name='subtree_lstm', train_data=subtree_train_data)\n",
        "# %%\n",
        "plt.plot(loss)\n"
      ],
      "execution_count": null,
      "metadata": {},
      "cell_type": "code"
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "language_info": {
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "version": "3.6.1",
      "name": "python",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4
}