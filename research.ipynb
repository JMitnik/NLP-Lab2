{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mefWoVI7FM2G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "plt.style.use('default')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gc4jd_CrFM2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "65e8ca30-e7ae-4e9a-98d9-3e1009b93dda"
      },
      "cell_type": "code",
      "source": [
        "!pip install humanize\n",
        "import psutil, humanize\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free:\" + humanize.naturalsize( psutil.virtual_memory().available ), \"| Proc size:\" + humanize.naturalsize( process.memory_info().rss))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VSLRYgGoFM2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "903d3ec1-ee6f-4a51-eced-a518171c9a35"
      },
      "cell_type": "code",
      "source": [
        "class MutePrint:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stdout_restore = sys.stdout\n",
        "    # Disable\n",
        "\n",
        "    def blockPrint(self):\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    # Restore\n",
        "    def enablePrint(self):\n",
        "        sys.stdout = self.stdout_restore\n",
        "mute = MutePrint()\n",
        "# %%\n",
        "!wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip -O trainDevTestTrees_PTB.zip\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-11 16:24:11--  http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip [following]\n",
            "--2018-12-11 16:24:11--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789539 (771K) [application/zip]\n",
            "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
            "\n",
            "trainDevTestTrees_P 100%[===================>] 771.03K  2.04MB/s    in 0.4s    \n",
            "\n",
            "2018-12-11 16:24:11 (2.04 MB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
            "\n",
            "Archive:  trainDevTestTrees_PTB.zip\n",
            "   creating: trees/\n",
            "  inflating: trees/dev.txt           \n",
            "  inflating: trees/test.txt          \n",
            "  inflating: trees/train.txt         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xHG9wRNfFM3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "972347a3-17f7-4865-c4ea-7d080840222e"
      },
      "cell_type": "code",
      "source": [
        "!pip install pytreebank\n",
        "import pytreebank\n",
        "# %%\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p | grep cudart.so | sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "# if accelerator == 'cpu':\n",
        "#   raise InvalidArgumentError('should run this notebook under gpu enviroment')\n",
        "# !pip install torch torchvision"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 28kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60ef6000 @  0x7f26bc0242a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2MV3xJP4FM36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "18d05870-91f5-4bb9-a518-9a106f27166d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" ."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "291p1pdHFM4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/JMitnik/NLP-Lab2/raw/cg/main.py -O ./main.py\n",
        "!wget -q https://github.com/JMitnik/NLP-Lab2/raw/cg/utils.py -O ./utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MkCgiPaZFM46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mute.blockPrint()\n",
        "from main import *\n",
        "from utils import *\n",
        "mute.enablePrint()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O63anO-oFM4_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_subtree_dataset():\n",
        "    '''\n",
        "    extract all subtrees together to the exact form of the `train_data` used in last ipynb\n",
        "    args: None\n",
        "    returns: a list contains three list of Examples, each of them corresponds to one of 'train', 'test', 'dev' set\n",
        "    '''\n",
        "    dataset = pytreebank.load_sst(\"./trees\")\n",
        "    datasets = dataset.values()\n",
        "    print(dataset.keys())\n",
        "    results = []\n",
        "    for D in datasets:\n",
        "        result = []\n",
        "        for tree in D:\n",
        "            tree.lowercase()\n",
        "            for c in tree.all_children():\n",
        "                sc = str(c)\n",
        "                trans = transitions_from_treestring(sc)\n",
        "                label = c.label\n",
        "                tree = c\n",
        "                tokens = tokens_from_treestring(sc)\n",
        "                result.append(Example(tokens=tokens, tree=tree, label=label, transitions=trans))\n",
        "        results.append(result)\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gm8K6bSTFM5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db8b567a-ec61-4ee0-9990-55ca71f1fb25"
      },
      "cell_type": "code",
      "source": [
        "subtree_train_data, subtree_test_data, subtree_dev_data = get_subtree_dataset()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'test', 'dev'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KwJd2DGKFM5N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Experiment():\n",
        "\n",
        "    def __init__(self, model, optimizer, *args, **kwargs):\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "    def train(self):\n",
        "        path = \"{}.pt\".format(self.kwargs['exp_name'] if 'exp_name' in self.kwargs else self.model.__class__.__name__)\n",
        "        if os.path.exists(path):\n",
        "            ckpt= torch.load(path)\n",
        "            self.model.load_state_dict(ckpt[\"state_dict\"])\n",
        "            return\n",
        "        self.losses, self.accs = train_model(self.model, self.optimizer, *self.args, **self.kwargs)\n",
        "    def eval(self, eval_fn=None, data=test_data, **kwargs):\n",
        "        if eval_fn is None:\n",
        "            if not ('eval_fn' in kwargs):\n",
        "                eval_fn = simple_evaluate\n",
        "            else:\n",
        "                eval_fn = self.kwargs['eval_fn'] \n",
        "        return eval_fn(self.model, data, **kwargs)\n",
        "\n",
        "    def plot(self):\n",
        "        plt.plot(self.losses)\n",
        "        plt.plot(self.accs)\n",
        "        return\n",
        "    def get_accuracy():\n",
        "        return self.accs\n",
        "\n",
        "    def get_losses():\n",
        "        return self.losses\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yucjsB_IFM5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_subtreelstm_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Returns sentences reversed (last word first)\n",
        "  Returns transitions together with the sentences.  \n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "    \n",
        "  # vocab returns 0 if the word is not there\n",
        "  # NOTE: reversed sequence!\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
        "  \n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "  \n",
        "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
        "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
        "  transitions = np.array(transitions)\n",
        "  transitions = transitions.T  # time-major\n",
        "  \n",
        "  return (x, transitions), y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pknyTPZFM5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build all the experiments by feeding corresponding parameters\n",
        "# cant think of cleaner way to do it :(\n",
        "\n",
        "name2class = {'bow': BOW, 'cbow': CBOW, 'deep_cbow': DeepCBOW, 'pt_deep_cbow': DeepCBOW, 'lstm': LSTMClassifier,\n",
        "              'tree_lstm': TreeLSTMClassifier, 'subtree_lstm': TreeLSTMClassifier}\n",
        "name2lr = {'bow': 5e-4, 'cbow': 5e-4, 'deep_cbow': 5e-4, 'pt_deep_cbow': 5e-4,\n",
        "           'lstm': 3e-4, 'tree_lstm': 2e-4, 'subtree_lstm': 2e-4}\n",
        "xargs_bow = dict(num_iterations=30000, print_every=1000, eval_every=1000)\n",
        "xargs_lstm = dict(num_iterations=25000, print_every=250, eval_every=1000)\n",
        "xargs_tree_lstm = dict(num_iterations=30000,\n",
        "                       print_every=250, eval_every=250,\n",
        "                       prep_fn=prepare_treelstm_minibatch,\n",
        "                       eval_fn=evaluate,\n",
        "                       batch_fn=get_minibatch,\n",
        "                       batch_size=25, eval_batch_size=25)\n",
        "xargs_subtree_lstm = dict(num_iterations=30000,\n",
        "                          print_every=250, eval_every=250,\n",
        "                          prep_fn=prepare_subtreelstm_minibatch,\n",
        "                          eval_fn=evaluate,\n",
        "                          batch_fn=get_minibatch,\n",
        "                          batch_size=25, eval_batch_size=25, train_data=subtree_train_data)\n",
        "name2xargs = {'bow': xargs_bow, 'cbow': xargs_bow, 'deep_cbow': xargs_bow, 'pt_deep_cbow': xargs_bow,\n",
        "              'lstm': xargs_lstm, 'tree_lstm': xargs_tree_lstm, 'subtree_lstm': xargs_subtree_lstm}\n",
        "\n",
        "bow_p = [vocab_size, n_classes, v]\n",
        "cbow_p = [len(v.w2i), embedding_dim, len(t2i), v]\n",
        "deep_cbow_p = [len(v.w2i), embedding_dim, hidden_dim, len(t2i), v]\n",
        "pt_deep_cbow_p = [len(nv.w2i), embedding_dim, hidden_dim, len(t2i), nv]\n",
        "lstm_p = [len(nv.w2i), 300, 168, len(t2i), nv]\n",
        "tree_lstm_p = [len(nv.w2i), 300, 150, len(t2i), nv]\n",
        "name2model_p = {'bow': bow_p, 'cbow': cbow_p, 'deep_cbow': deep_cbow_p, 'pt_deep_cbow': pt_deep_cbow_p,\n",
        "                'lstm': lstm_p, 'tree_lstm': tree_lstm_p, 'subtree_lstm': tree_lstm_p}\n",
        "\n",
        "\n",
        "def do_experiment(rd_seed, exp_name_li=list(name2class.keys())):\n",
        "    torch.cuda.manual_seed(rd_seed)\n",
        "    np.random.seed(rd_seed)\n",
        "    for n in exp_name_li:\n",
        "        # build a new tree lstm for feeding subtree\n",
        "        model = name2class[n](*name2model_p[n])\n",
        "        model = model.to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=name2lr[n])\n",
        "        if n.startswith('pt') or n.endswith('lstm'):\n",
        "            with torch.no_grad():\n",
        "                model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "                model.embed.weight.requires_grad = False\n",
        "        exp = Experiment(model, optimizer, exp_name='{}_rd_seed_{}'.format(\n",
        "            n, rd_seed), **name2xargs[n])\n",
        "        exp.train()\n",
        "        yield exp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "glre-nfdFM5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7582
        },
        "outputId": "be6a7eee-f7ed-4884-fb2c-622f6aa30c32"
      },
      "cell_type": "code",
      "source": [
        "rd_s_li = [7, 42, 1984]\n",
        "exp_li = []\n",
        "for rs_s in rd_s_li:\n",
        "    exp_li.append(list(do_experiment(rs_s)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuffling training data\n",
            "Iter 1000: loss=6122.6316, time=1.93s\n",
            "iter 1000: dev acc=0.2171\n",
            "new highscore\n",
            "Iter 2000: loss=5426.2721, time=4.51s\n",
            "iter 2000: dev acc=0.2298\n",
            "new highscore\n",
            "Iter 3000: loss=5295.1953, time=6.99s\n",
            "iter 3000: dev acc=0.2343\n",
            "new highscore\n",
            "Iter 4000: loss=5113.4322, time=9.41s\n",
            "iter 4000: dev acc=0.2380\n",
            "new highscore\n",
            "Iter 5000: loss=4972.0239, time=11.86s\n",
            "iter 5000: dev acc=0.2389\n",
            "new highscore\n",
            "Iter 6000: loss=4747.8348, time=14.30s\n",
            "iter 6000: dev acc=0.2352\n",
            "Iter 7000: loss=4823.1034, time=16.74s\n",
            "iter 7000: dev acc=0.2352\n",
            "Iter 8000: loss=4666.4653, time=19.17s\n",
            "iter 8000: dev acc=0.2352\n",
            "Shuffling training data\n",
            "Iter 9000: loss=4693.8968, time=21.61s\n",
            "iter 9000: dev acc=0.2398\n",
            "new highscore\n",
            "Iter 10000: loss=4576.0128, time=24.05s\n",
            "iter 10000: dev acc=0.2380\n",
            "Iter 11000: loss=4151.6676, time=26.49s\n",
            "iter 11000: dev acc=0.2371\n",
            "Iter 12000: loss=4142.7373, time=28.92s\n",
            "iter 12000: dev acc=0.2416\n",
            "new highscore\n",
            "Iter 13000: loss=4195.7678, time=31.36s\n",
            "iter 13000: dev acc=0.2443\n",
            "new highscore\n",
            "Iter 14000: loss=3961.3246, time=33.80s\n",
            "iter 14000: dev acc=0.2425\n",
            "Iter 15000: loss=4202.3195, time=36.23s\n",
            "iter 15000: dev acc=0.2425\n",
            "Iter 16000: loss=3976.0677, time=38.66s\n",
            "iter 16000: dev acc=0.2416\n",
            "Iter 17000: loss=4065.5984, time=41.11s\n",
            "iter 17000: dev acc=0.2416\n",
            "Shuffling training data\n",
            "Iter 18000: loss=3793.1936, time=43.55s\n",
            "iter 18000: dev acc=0.2525\n",
            "new highscore\n",
            "Iter 19000: loss=3870.0024, time=45.98s\n",
            "iter 19000: dev acc=0.2470\n",
            "Iter 20000: loss=3709.8630, time=48.41s\n",
            "iter 20000: dev acc=0.2489\n",
            "Iter 21000: loss=3615.2100, time=50.84s\n",
            "iter 21000: dev acc=0.2470\n",
            "Iter 22000: loss=3490.2077, time=53.40s\n",
            "iter 22000: dev acc=0.2452\n",
            "Iter 23000: loss=3635.2712, time=55.96s\n",
            "iter 23000: dev acc=0.2525\n",
            "Iter 24000: loss=3332.1064, time=58.53s\n",
            "iter 24000: dev acc=0.2543\n",
            "new highscore\n",
            "Iter 25000: loss=3324.6365, time=61.10s\n",
            "iter 25000: dev acc=0.2561\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 26000: loss=3373.7715, time=63.56s\n",
            "iter 26000: dev acc=0.2561\n",
            "Iter 27000: loss=3204.8367, time=65.98s\n",
            "iter 27000: dev acc=0.2589\n",
            "new highscore\n",
            "Iter 28000: loss=3218.4024, time=68.42s\n",
            "iter 28000: dev acc=0.2589\n",
            "Iter 29000: loss=3191.8433, time=70.85s\n",
            "iter 29000: dev acc=0.2589\n",
            "Iter 30000: loss=3177.7757, time=73.28s\n",
            "iter 30000: dev acc=0.2598\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 30000: train acc=0.3174, dev acc=0.2598, test acc=0.2538\n",
            "Shuffling training data\n",
            "Iter 1000: loss=2665.0584, time=4.90s\n",
            "iter 1000: dev acc=0.2271\n",
            "new highscore\n",
            "Iter 2000: loss=2222.0294, time=10.60s\n",
            "iter 2000: dev acc=0.2779\n",
            "new highscore\n",
            "Iter 3000: loss=2143.2531, time=16.32s\n",
            "iter 3000: dev acc=0.2734\n",
            "Iter 4000: loss=2008.3490, time=21.96s\n",
            "iter 4000: dev acc=0.2788\n",
            "new highscore\n",
            "Iter 5000: loss=1928.6908, time=27.70s\n",
            "iter 5000: dev acc=0.2979\n",
            "new highscore\n",
            "Iter 6000: loss=1902.6661, time=33.45s\n",
            "iter 6000: dev acc=0.2770\n",
            "Iter 7000: loss=1895.6605, time=39.10s\n",
            "iter 7000: dev acc=0.2906\n",
            "Iter 8000: loss=1793.8728, time=44.77s\n",
            "iter 8000: dev acc=0.3115\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1722.1907, time=50.53s\n",
            "iter 9000: dev acc=0.3070\n",
            "Iter 10000: loss=1553.8801, time=56.20s\n",
            "iter 10000: dev acc=0.3061\n",
            "Iter 11000: loss=1629.6994, time=61.86s\n",
            "iter 11000: dev acc=0.3197\n",
            "new highscore\n",
            "Iter 12000: loss=1647.1490, time=67.62s\n",
            "iter 12000: dev acc=0.3224\n",
            "new highscore\n",
            "Iter 13000: loss=1606.9255, time=73.39s\n",
            "iter 13000: dev acc=0.3270\n",
            "new highscore\n",
            "Iter 14000: loss=1661.3732, time=79.15s\n",
            "iter 14000: dev acc=0.3134\n",
            "Iter 15000: loss=1595.7149, time=84.82s\n",
            "iter 15000: dev acc=0.3206\n",
            "Iter 16000: loss=1665.4201, time=90.48s\n",
            "iter 16000: dev acc=0.3315\n",
            "new highscore\n",
            "Iter 17000: loss=1634.3634, time=96.25s\n",
            "iter 17000: dev acc=0.3179\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1280.6727, time=101.92s\n",
            "iter 18000: dev acc=0.3370\n",
            "new highscore\n",
            "Iter 19000: loss=1293.9005, time=107.68s\n",
            "iter 19000: dev acc=0.3333\n",
            "Iter 20000: loss=1309.7086, time=113.35s\n",
            "iter 20000: dev acc=0.3243\n",
            "Iter 21000: loss=1285.0718, time=119.01s\n",
            "iter 21000: dev acc=0.3379\n",
            "new highscore\n",
            "Iter 22000: loss=1393.8159, time=124.77s\n",
            "iter 22000: dev acc=0.3297\n",
            "Iter 23000: loss=1292.7212, time=130.43s\n",
            "iter 23000: dev acc=0.3588\n",
            "new highscore\n",
            "Iter 24000: loss=1333.3043, time=136.19s\n",
            "iter 24000: dev acc=0.3306\n",
            "Iter 25000: loss=1335.5175, time=141.88s\n",
            "iter 25000: dev acc=0.3351\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1226.9052, time=147.55s\n",
            "iter 26000: dev acc=0.3470\n",
            "Iter 27000: loss=942.5003, time=153.22s\n",
            "iter 27000: dev acc=0.3179\n",
            "Iter 28000: loss=994.5872, time=158.89s\n",
            "iter 28000: dev acc=0.3333\n",
            "Iter 29000: loss=991.4372, time=164.56s\n",
            "iter 29000: dev acc=0.3261\n",
            "Iter 30000: loss=1101.1560, time=170.23s\n",
            "iter 30000: dev acc=0.3170\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 23000: train acc=0.6277, dev acc=0.3588, test acc=0.3824\n",
            "Shuffling training data\n",
            "Iter 1000: loss=1594.5550, time=5.94s\n",
            "iter 1000: dev acc=0.2825\n",
            "new highscore\n",
            "Iter 2000: loss=1574.0986, time=13.05s\n",
            "iter 2000: dev acc=0.2888\n",
            "new highscore\n",
            "Iter 3000: loss=1533.0235, time=20.13s\n",
            "iter 3000: dev acc=0.3097\n",
            "new highscore\n",
            "Iter 4000: loss=1554.7695, time=27.25s\n",
            "iter 4000: dev acc=0.3152\n",
            "new highscore\n",
            "Iter 5000: loss=1530.6821, time=34.37s\n",
            "iter 5000: dev acc=0.3270\n",
            "new highscore\n",
            "Iter 6000: loss=1532.3031, time=41.49s\n",
            "iter 6000: dev acc=0.3451\n",
            "new highscore\n",
            "Iter 7000: loss=1504.7789, time=48.68s\n",
            "iter 7000: dev acc=0.3197\n",
            "Iter 8000: loss=1480.7554, time=55.74s\n",
            "iter 8000: dev acc=0.3460\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1462.0532, time=62.88s\n",
            "iter 9000: dev acc=0.3542\n",
            "new highscore\n",
            "Iter 10000: loss=1376.0856, time=70.01s\n",
            "iter 10000: dev acc=0.3551\n",
            "new highscore\n",
            "Iter 11000: loss=1390.8471, time=77.15s\n",
            "iter 11000: dev acc=0.3488\n",
            "Iter 12000: loss=1375.9240, time=84.18s\n",
            "iter 12000: dev acc=0.3651\n",
            "new highscore\n",
            "Iter 13000: loss=1350.0240, time=91.29s\n",
            "iter 13000: dev acc=0.3442\n",
            "Iter 14000: loss=1316.7479, time=98.31s\n",
            "iter 14000: dev acc=0.3597\n",
            "Iter 15000: loss=1318.8679, time=105.41s\n",
            "iter 15000: dev acc=0.3370\n",
            "Iter 16000: loss=1344.4094, time=112.54s\n",
            "iter 16000: dev acc=0.3542\n",
            "Iter 17000: loss=1349.9958, time=119.57s\n",
            "iter 17000: dev acc=0.3460\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1138.7315, time=126.60s\n",
            "iter 18000: dev acc=0.3515\n",
            "Iter 19000: loss=1183.7785, time=133.63s\n",
            "iter 19000: dev acc=0.3733\n",
            "new highscore\n",
            "Iter 20000: loss=1167.9202, time=140.76s\n",
            "iter 20000: dev acc=0.3488\n",
            "Iter 21000: loss=1147.0497, time=147.78s\n",
            "iter 21000: dev acc=0.3697\n",
            "Iter 22000: loss=1214.6738, time=154.81s\n",
            "iter 22000: dev acc=0.3297\n",
            "Iter 23000: loss=1158.1643, time=161.85s\n",
            "iter 23000: dev acc=0.3479\n",
            "Iter 24000: loss=1192.9822, time=168.86s\n",
            "iter 24000: dev acc=0.3451\n",
            "Iter 25000: loss=1217.7651, time=175.88s\n",
            "iter 25000: dev acc=0.3606\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1116.1423, time=182.92s\n",
            "iter 26000: dev acc=0.3642\n",
            "Iter 27000: loss=968.9854, time=189.94s\n",
            "iter 27000: dev acc=0.3588\n",
            "Iter 28000: loss=945.7929, time=196.97s\n",
            "iter 28000: dev acc=0.3642\n",
            "Iter 29000: loss=977.8315, time=204.00s\n",
            "iter 29000: dev acc=0.3424\n",
            "Iter 30000: loss=1058.9110, time=211.02s\n",
            "iter 30000: dev acc=0.3597\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 19000: train acc=0.5393, dev acc=0.3733, test acc=0.3778\n",
            "Shuffling training data\n",
            "Iter 1000: loss=1506.4152, time=3.98s\n",
            "iter 1000: dev acc=0.3460\n",
            "new highscore\n",
            "Iter 2000: loss=1419.9665, time=9.09s\n",
            "iter 2000: dev acc=0.4060\n",
            "new highscore\n",
            "Iter 3000: loss=1353.3127, time=14.21s\n",
            "iter 3000: dev acc=0.3951\n",
            "Iter 4000: loss=1346.2126, time=19.28s\n",
            "iter 4000: dev acc=0.3597\n",
            "Iter 5000: loss=1367.5145, time=24.35s\n",
            "iter 5000: dev acc=0.3760\n",
            "Iter 6000: loss=1346.6476, time=29.44s\n",
            "iter 6000: dev acc=0.4033\n",
            "Iter 7000: loss=1306.4730, time=34.52s\n",
            "iter 7000: dev acc=0.4105\n",
            "new highscore\n",
            "Iter 8000: loss=1316.3404, time=39.65s\n",
            "iter 8000: dev acc=0.4051\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1295.9123, time=44.75s\n",
            "iter 9000: dev acc=0.4142\n",
            "new highscore\n",
            "Iter 10000: loss=1290.0491, time=49.89s\n",
            "iter 10000: dev acc=0.3987\n",
            "Iter 11000: loss=1314.6146, time=54.99s\n",
            "iter 11000: dev acc=0.3969\n",
            "Iter 12000: loss=1310.8440, time=60.08s\n",
            "iter 12000: dev acc=0.4296\n",
            "new highscore\n",
            "Iter 13000: loss=1311.6658, time=65.20s\n",
            "iter 13000: dev acc=0.4005\n",
            "Iter 14000: loss=1290.4472, time=70.29s\n",
            "iter 14000: dev acc=0.4060\n",
            "Iter 15000: loss=1282.8987, time=75.38s\n",
            "iter 15000: dev acc=0.4278\n",
            "Iter 16000: loss=1309.3481, time=80.46s\n",
            "iter 16000: dev acc=0.4205\n",
            "Iter 17000: loss=1290.5829, time=85.54s\n",
            "iter 17000: dev acc=0.4151\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1262.3841, time=90.66s\n",
            "iter 18000: dev acc=0.4069\n",
            "Iter 19000: loss=1268.1252, time=95.73s\n",
            "iter 19000: dev acc=0.4042\n",
            "Iter 20000: loss=1273.7997, time=100.82s\n",
            "iter 20000: dev acc=0.4260\n",
            "Iter 21000: loss=1293.1044, time=105.90s\n",
            "iter 21000: dev acc=0.4187\n",
            "Iter 22000: loss=1273.9288, time=110.99s\n",
            "iter 22000: dev acc=0.4342\n",
            "new highscore\n",
            "Iter 23000: loss=1243.1359, time=116.12s\n",
            "iter 23000: dev acc=0.4024\n",
            "Iter 24000: loss=1284.1461, time=121.21s\n",
            "iter 24000: dev acc=0.4369\n",
            "new highscore\n",
            "Iter 25000: loss=1316.9997, time=126.34s\n",
            "iter 25000: dev acc=0.4287\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1257.2518, time=131.66s\n",
            "iter 26000: dev acc=0.4196\n",
            "Iter 27000: loss=1230.0997, time=136.75s\n",
            "iter 27000: dev acc=0.4214\n",
            "Iter 28000: loss=1229.8526, time=141.83s\n",
            "iter 28000: dev acc=0.3924\n",
            "Iter 29000: loss=1268.2783, time=146.92s\n",
            "iter 29000: dev acc=0.4196\n",
            "Iter 30000: loss=1241.3977, time=152.03s\n",
            "iter 30000: dev acc=0.4178\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 24000: train acc=0.4645, dev acc=0.4369, test acc=0.4443\n",
            "Shuffling training data\n",
            "Iter 250: loss=388.9530, time=8.52s\n",
            "Iter 500: loss=393.9883, time=16.72s\n",
            "Iter 750: loss=380.9929, time=25.63s\n",
            "Iter 1000: loss=366.5202, time=34.16s\n",
            "iter 1000: dev acc=0.3660\n",
            "new highscore\n",
            "Iter 1250: loss=360.2600, time=57.13s\n",
            "Iter 1500: loss=348.2900, time=65.72s\n",
            "Iter 1750: loss=360.7363, time=73.84s\n",
            "Iter 2000: loss=352.9219, time=82.16s\n",
            "iter 2000: dev acc=0.3833\n",
            "new highscore\n",
            "Iter 2250: loss=351.8796, time=105.36s\n",
            "Iter 2500: loss=344.0272, time=113.52s\n",
            "Iter 2750: loss=353.5744, time=122.12s\n",
            "Iter 3000: loss=336.2356, time=130.39s\n",
            "iter 3000: dev acc=0.3942\n",
            "new highscore\n",
            "Iter 3250: loss=341.1364, time=153.48s\n",
            "Iter 3500: loss=344.0587, time=162.20s\n",
            "Iter 3750: loss=338.0500, time=170.73s\n",
            "Iter 4000: loss=340.4397, time=178.98s\n",
            "iter 4000: dev acc=0.3924\n",
            "Iter 4250: loss=331.4922, time=202.10s\n",
            "Iter 4500: loss=337.3635, time=210.57s\n",
            "Iter 4750: loss=339.3601, time=218.78s\n",
            "Iter 5000: loss=319.5296, time=227.14s\n",
            "iter 5000: dev acc=0.4178\n",
            "new highscore\n",
            "Iter 5250: loss=351.2692, time=250.13s\n",
            "Iter 5500: loss=317.0625, time=258.47s\n",
            "Iter 5750: loss=330.9343, time=267.15s\n",
            "Iter 6000: loss=330.8007, time=275.59s\n",
            "iter 6000: dev acc=0.4178\n",
            "Iter 6250: loss=330.5541, time=298.62s\n",
            "Iter 6500: loss=331.1764, time=307.03s\n",
            "Iter 6750: loss=331.6984, time=315.56s\n",
            "Iter 7000: loss=335.0433, time=323.73s\n",
            "iter 7000: dev acc=0.4033\n",
            "Iter 7250: loss=325.2106, time=347.22s\n",
            "Iter 7500: loss=325.4975, time=355.21s\n",
            "Iter 7750: loss=328.8576, time=363.92s\n",
            "Iter 8000: loss=344.5843, time=372.29s\n",
            "iter 8000: dev acc=0.4251\n",
            "new highscore\n",
            "Iter 8250: loss=326.8916, time=395.81s\n",
            "Iter 8500: loss=335.3170, time=404.09s\n",
            "Shuffling training data\n",
            "Iter 8750: loss=323.0086, time=412.42s\n",
            "Iter 9000: loss=304.7019, time=420.87s\n",
            "iter 9000: dev acc=0.4278\n",
            "new highscore\n",
            "Iter 9250: loss=323.3538, time=444.37s\n",
            "Iter 9500: loss=300.5090, time=452.60s\n",
            "Iter 9750: loss=322.3229, time=461.21s\n",
            "Iter 10000: loss=315.4173, time=469.46s\n",
            "iter 10000: dev acc=0.4069\n",
            "Iter 10250: loss=331.9695, time=492.76s\n",
            "Iter 10500: loss=324.3669, time=501.27s\n",
            "Iter 10750: loss=326.2684, time=509.51s\n",
            "Iter 11000: loss=324.9149, time=517.93s\n",
            "iter 11000: dev acc=0.4033\n",
            "Iter 11250: loss=304.6929, time=540.72s\n",
            "Iter 11500: loss=317.2182, time=548.92s\n",
            "Iter 11750: loss=306.9104, time=557.49s\n",
            "Iter 12000: loss=313.9709, time=565.87s\n",
            "iter 12000: dev acc=0.4487\n",
            "new highscore\n",
            "Iter 12250: loss=308.9284, time=589.13s\n",
            "Iter 12500: loss=308.1198, time=597.51s\n",
            "Iter 12750: loss=325.0348, time=606.15s\n",
            "Iter 13000: loss=325.9592, time=614.86s\n",
            "iter 13000: dev acc=0.4496\n",
            "new highscore\n",
            "Iter 13250: loss=322.2563, time=638.04s\n",
            "Iter 13500: loss=312.3082, time=646.41s\n",
            "Iter 13750: loss=307.1117, time=655.18s\n",
            "Iter 14000: loss=298.6847, time=663.48s\n",
            "iter 14000: dev acc=0.4414\n",
            "Iter 14250: loss=300.6325, time=686.31s\n",
            "Iter 14500: loss=318.3493, time=694.51s\n",
            "Iter 14750: loss=320.7171, time=702.78s\n",
            "Iter 15000: loss=311.1161, time=710.73s\n",
            "iter 15000: dev acc=0.4351\n",
            "Iter 15250: loss=305.1807, time=733.65s\n",
            "Iter 15500: loss=315.2615, time=742.17s\n",
            "Iter 15750: loss=298.1528, time=750.83s\n",
            "Iter 16000: loss=300.8645, time=759.00s\n",
            "iter 16000: dev acc=0.4360\n",
            "Iter 16250: loss=314.6971, time=782.32s\n",
            "Iter 16500: loss=313.8447, time=790.53s\n",
            "Iter 16750: loss=316.0495, time=799.23s\n",
            "Iter 17000: loss=289.3437, time=807.53s\n",
            "iter 17000: dev acc=0.4369\n",
            "Shuffling training data\n",
            "Iter 17250: loss=299.8469, time=830.41s\n",
            "Iter 17500: loss=285.1176, time=838.81s\n",
            "Iter 17750: loss=280.1351, time=847.14s\n",
            "Iter 18000: loss=273.3350, time=855.40s\n",
            "iter 18000: dev acc=0.4496\n",
            "Iter 18250: loss=286.7684, time=878.58s\n",
            "Iter 18500: loss=292.9092, time=887.60s\n",
            "Iter 18750: loss=286.8236, time=895.96s\n",
            "Iter 19000: loss=310.5756, time=904.13s\n",
            "iter 19000: dev acc=0.4623\n",
            "new highscore\n",
            "Iter 19250: loss=298.1481, time=927.15s\n",
            "Iter 19500: loss=301.0499, time=935.95s\n",
            "Iter 19750: loss=304.0136, time=944.12s\n",
            "Iter 20000: loss=281.1042, time=952.66s\n",
            "iter 20000: dev acc=0.4532\n",
            "Iter 20250: loss=295.7703, time=976.17s\n",
            "Iter 20500: loss=320.8750, time=984.55s\n",
            "Iter 20750: loss=299.4048, time=992.93s\n",
            "Iter 21000: loss=295.4706, time=1001.31s\n",
            "iter 21000: dev acc=0.4496\n",
            "Iter 21250: loss=275.4568, time=1024.43s\n",
            "Iter 21500: loss=291.0371, time=1032.99s\n",
            "Iter 21750: loss=276.6581, time=1041.15s\n",
            "Iter 22000: loss=289.7578, time=1049.62s\n",
            "iter 22000: dev acc=0.4614\n",
            "Iter 22250: loss=302.6506, time=1072.65s\n",
            "Iter 22500: loss=298.2107, time=1081.15s\n",
            "Iter 22750: loss=307.6976, time=1089.74s\n",
            "Iter 23000: loss=295.3295, time=1098.28s\n",
            "iter 23000: dev acc=0.4496\n",
            "Iter 23250: loss=302.1822, time=1121.36s\n",
            "Iter 23500: loss=301.7292, time=1129.53s\n",
            "Iter 23750: loss=300.1125, time=1137.78s\n",
            "Iter 24000: loss=296.5403, time=1146.29s\n",
            "iter 24000: dev acc=0.4659\n",
            "new highscore\n",
            "Iter 24250: loss=288.5751, time=1169.67s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K6Rcj0GZFM5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(accelerator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHkfsCZ1FdX7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}